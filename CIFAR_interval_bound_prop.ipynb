{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_interval_bound_prop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawelmorawiecki/ABIDE_experiments/blob/master/CIFAR_interval_bound_prop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UoK0sWEypQ-s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Osqh5JlNpQ-2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "import torch.utils.data as data_utils\n",
        "#from utils import epoch, epoch_robust_bound, epoch_calculate_robust_err, Flatten, generate_kappa_schedule_CIFAR, generate_epsilon_schedule_CIFAR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuWv5TZpukKi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils and helper functions"
      ]
    },
    {
      "metadata": {
        "id": "1XdcOwIVuv7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1) \n",
        "\n",
        "def epoch(loader, model, device, opt=None):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp,_ = model(X)\n",
        "        loss = nn.CrossEntropyLoss()(yp,y)\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def bound_propagation(model, initial_bound):\n",
        "    l, u = initial_bound\n",
        "    bounds = []\n",
        "    bounds.append(initial_bound)\n",
        "    list_of_layers = list(model.children())\n",
        "    \n",
        "    for i in range(len(list_of_layers)):\n",
        "        layer = list_of_layers[i]\n",
        "        \n",
        "        if isinstance(layer, Flatten):\n",
        "            l_ = Flatten()(l)\n",
        "            u_ = Flatten()(u)\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            l_ = (layer.weight.clamp(min=0) @ l.t() + layer.weight.clamp(max=0) @ u.t() \n",
        "                  + layer.bias[:,None]).t()\n",
        "            u_ = (layer.weight.clamp(min=0) @ u.t() + layer.weight.clamp(max=0) @ l.t() \n",
        "                  + layer.bias[:,None]).t()\n",
        "            \n",
        "        elif isinstance(layer, nn.Conv2d):\n",
        "            l_ = (nn.functional.conv2d(l, layer.weight.clamp(min=0), bias=None, \n",
        "                                       stride=layer.stride, padding=layer.padding,\n",
        "                                       dilation=layer.dilation, groups=layer.groups) +\n",
        "                  nn.functional.conv2d(u, layer.weight.clamp(max=0), bias=None, \n",
        "                                       stride=layer.stride, padding=layer.padding,\n",
        "                                       dilation=layer.dilation, groups=layer.groups) +\n",
        "                  layer.bias[None,:,None,None])\n",
        "            \n",
        "            u_ = (nn.functional.conv2d(u, layer.weight.clamp(min=0), bias=None, \n",
        "                                       stride=layer.stride, padding=layer.padding,\n",
        "                                       dilation=layer.dilation, groups=layer.groups) +\n",
        "                  nn.functional.conv2d(l, layer.weight.clamp(max=0), bias=None, \n",
        "                                       stride=layer.stride, padding=layer.padding,\n",
        "                                       dilation=layer.dilation, groups=layer.groups) + \n",
        "                  layer.bias[None,:,None,None])\n",
        "            \n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            l_ = l.clamp(min=0)\n",
        "            u_ = u.clamp(min=0)\n",
        "            \n",
        "        bounds.append((l_, u_))\n",
        "        l,u = l_, u_\n",
        "    return bounds\n",
        "\n",
        "\n",
        "def interval_based_bound(model, c, bounds, idx):\n",
        "    # requires last layer to be linear\n",
        "    cW = c.t() @ model.last_linear.weight\n",
        "    cb = c.t() @ model.last_linear.bias\n",
        "    \n",
        "    l,u = bounds[-2]\n",
        "    return (cW.clamp(min=0) @ l[idx].t() + cW.clamp(max=0) @ u[idx].t() + cb[:,None]).t()\n",
        "\n",
        "\n",
        "def epoch_robust_bound(loader, model, epsilon_schedule, device, kappa_schedule, batch_counter, opt=None):\n",
        "    robust_err = 0\n",
        "    total_robust_loss = 0\n",
        "    total_mse_loss = 0\n",
        "    total_combined_loss = 0\n",
        "    \n",
        "    C = [-torch.eye(10).to(device) for _ in range(10)]\n",
        "    for y0 in range(10):\n",
        "        C[y0][y0,:] += 1\n",
        "    \n",
        "    for i,data in enumerate(loader,0):\n",
        "      \n",
        "        if i>99:  #calculate only for 100 batches\n",
        "          break      \n",
        "        \n",
        "        mse_loss_list = []\n",
        "        lower_bounds = []\n",
        "        upper_bounds = []\n",
        "        \n",
        "        \n",
        "        X,y = data\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        \n",
        "        ###### fit loss calculation ######\n",
        "        yp,_ = model(X)\n",
        "        fit_loss = nn.CrossEntropyLoss()(yp,y)\n",
        "    \n",
        "        ###### robust loss calculation ######\n",
        "        initial_bound = (X - epsilon_schedule[batch_counter], X + epsilon_schedule[batch_counter])\n",
        "        bounds = bound_propagation(model, initial_bound)\n",
        "        robust_loss = 0\n",
        "        for y0 in range(10):\n",
        "            if sum(y==y0) > 0:\n",
        "                lower_bound = interval_based_bound(model, C[y0], bounds, y==y0)\n",
        "                robust_loss += nn.CrossEntropyLoss(reduction='sum')(-lower_bound, y[y==y0]) / X.shape[0]\n",
        "                \n",
        "                robust_err += (lower_bound.min(dim=1)[0] < 0).sum().item() #increment when true label is not winning       \n",
        "        \n",
        "        total_robust_loss += robust_loss.item() * X.shape[0]  \n",
        "        \n",
        "        ##### MSE Loss #####\n",
        "        \n",
        "        #indices_of_layers = [2,4,7,8] #CNN_small\n",
        "        indices_of_layers = [2,4,6,8,11,13,14] #CNN_medium\n",
        "        \n",
        "        \n",
        "        for i in range(len(indices_of_layers)):\n",
        "            lower_bounds.append(Flatten()(bounds[indices_of_layers[i]][0])) #lower bounds \n",
        "            upper_bounds.append(Flatten()(bounds[indices_of_layers[i]][1])) #upper bounds \n",
        "            mse_loss_list.append(nn.MSELoss()(lower_bounds[i], upper_bounds[i]))\n",
        "        \n",
        "        mse_loss = mse_loss_list[0] + mse_loss_list[1] + mse_loss_list[2] + mse_loss_list[3] + mse_loss_list[4] + mse_loss_list[5] + mse_loss_list[6]\n",
        "        total_mse_loss += mse_loss.item()\n",
        "        \n",
        "        ###### combined losss ######\n",
        "        combined_loss = kappa_schedule[batch_counter]*fit_loss + (1-kappa_schedule[batch_counter])*robust_loss + mse_loss\n",
        "        total_combined_loss += combined_loss.item()\n",
        "        \n",
        "        batch_counter +=1\n",
        "         \n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            combined_loss.backward()\n",
        "            opt.step() \n",
        "        \n",
        "    return robust_err / len(loader.dataset), total_combined_loss / len(loader.dataset), total_mse_loss/ len(loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "def new_epoch_robust_bound(loader, model, epsilon, alpha, device, opt=None):\n",
        "    \n",
        "    robust_err = 0\n",
        "    total_robust_loss = 0\n",
        "    total_fit_loss = 0\n",
        "    \n",
        "    C = [-torch.eye(10).to(device) for _ in range(10)]\n",
        "    for y0 in range(10):\n",
        "        C[y0][y0,:] += 1\n",
        "\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        \n",
        "        ###### fit loss calculation ######\n",
        "        yp,_ = model(X)\n",
        "        fit_loss = nn.CrossEntropyLoss()(yp,y)\n",
        "    \n",
        "        ###### robust loss calculation ######\n",
        "        initial_bound = (X - epsilon, X + epsilon)\n",
        "        bounds = bound_propagation(model, initial_bound, how_many_layers=14)\n",
        "        robust_loss = 0\n",
        "        for y0 in range(10):\n",
        "            if sum(y==y0) > 0:\n",
        "                lower_bound = interval_based_bound(model, C[y0], bounds, y==y0)\n",
        "                robust_loss += nn.CrossEntropyLoss(reduction='sum')(-lower_bound, y[y==y0]) / X.shape[0]        \n",
        "                robust_err += (lower_bound.min(dim=1)[0] < 0).sum().item() #increment when true label is not winning       \n",
        "        \n",
        "        total_robust_loss += robust_loss.item() * X.shape[0]\n",
        "        total_fit_loss += fit_loss.item() * X.shape[0]\n",
        "        \n",
        "                ###### combined losss ######\n",
        "        combined_loss = (1-alpha)*fit_loss + alpha*robust_loss\n",
        "      \n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            combined_loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "    return total_fit_loss / len(loader.dataset), total_robust_loss / len(loader.dataset)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "def epoch_calculate_robust_err (loader, model, epsilon, device):\n",
        "    robust_err = 0.0\n",
        "    \n",
        "    C = [-torch.eye(10).to(device) for _ in range(10)]\n",
        "    for y0 in range(10):\n",
        "        C[y0][y0,:] += 1\n",
        "\n",
        "\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        \n",
        "        initial_bound = (X - epsilon, X + epsilon)\n",
        "        bounds = bound_propagation(model, initial_bound)\n",
        "\n",
        "        for y0 in range(10):\n",
        "            if sum(y==y0) > 0:\n",
        "                lower_bound = interval_based_bound(model, C[y0], bounds, y==y0)                \n",
        "                robust_err += (lower_bound.min(dim=1)[0] < 0).sum().item() #increment when true label is not winning       \n",
        "        \n",
        "    return robust_err / len(loader.dataset)\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "def generate_kappa_schedule_MNIST():\n",
        "\n",
        "    kappa_schedule = 2000*[1] # warm-up phase\n",
        "    kappa_value = 1.0\n",
        "    step = 0.5/58000\n",
        "    \n",
        "    for i in range(58000):\n",
        "        kappa_value = kappa_value - step\n",
        "        kappa_schedule.append(kappa_value)\n",
        "    \n",
        "    return kappa_schedule\n",
        "\n",
        "def generate_epsilon_schedule_MNIST(epsilon_train):\n",
        "    \n",
        "    epsilon_schedule = []\n",
        "    step = epsilon_train/10000\n",
        "            \n",
        "    for i in range(10000):\n",
        "        epsilon_schedule.append(i*step) #ramp-up phase\n",
        "    \n",
        "    for i in range(50000):\n",
        "        epsilon_schedule.append(epsilon_train)\n",
        "        \n",
        "    return epsilon_schedule\n",
        "\n",
        "\n",
        "def generate_kappa_schedule_CIFAR():\n",
        "\n",
        "    kappa_schedule = 10000*[1] # warm-up phase\n",
        "    kappa_value = 1.0\n",
        "    step = 0.5/340000\n",
        "    \n",
        "    for i in range(340000):\n",
        "        kappa_value = kappa_value - step\n",
        "        kappa_schedule.append(kappa_value)\n",
        "    \n",
        "    return kappa_schedule\n",
        "\n",
        "def generate_epsilon_schedule_CIFAR(epsilon_train):\n",
        "    \n",
        "    epsilon_schedule = []\n",
        "    step = epsilon_train/150000\n",
        "            \n",
        "    for i in range(150000):\n",
        "        epsilon_schedule.append(i*step) #ramp-up phase\n",
        "    \n",
        "    for i in range(200000):\n",
        "        epsilon_schedule.append(epsilon_train)\n",
        "        \n",
        "    return epsilon_schedule "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7bW3yTVvMLW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jMEAJ22NpQ-9",
        "outputId": "585c3a2c-2280-4b07-bf25-5973331d2adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1231366050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "qqxdD66ymDYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 50\n",
        "dataset_path = './cifar10'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZdtPZTAOmDYR",
        "colab_type": "code",
        "outputId": "8db488ae-6a5c-47ba-c0aa-a982767b5176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "trainset = datasets.CIFAR10(root=dataset_path, train=True, download=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170352640/170498071 [00:39<00:00, 8792945.74it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xMR9U4qRmDYW",
        "colab_type": "code",
        "outputId": "87c9f44a-c086-4b83-92c4-af738baba931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "train_mean = trainset.data.mean(axis=(0,1,2))/255  # [0.49139968  0.48215841  0.44653091]\n",
        "train_std = trainset.data.std(axis=(0,1,2))/255  # [0.24703223  0.24348513  0.26158784]\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(train_mean, train_std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(train_mean, train_std),\n",
        "])\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(\n",
        "    root=dataset_path, train=True, download=True,\n",
        "    transform=transform_train),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=dataset_path, train=False, download=True,\n",
        "    transform=transform_test),\n",
        "    batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YfmN2PQb1nuO",
        "colab_type": "code",
        "outputId": "a898208b-cbde-4fe5-c6e0-f2232c5e119a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8CJe2L46pQ_z"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "HGimdiOImLDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_small(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(CNN_small, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 16, 4, padding=0, stride=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(16, 32, 4, padding=0, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.flat = Flatten()\n",
        "        self.linear1 = nn.Linear(32*12*12, 100)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.last_linear = nn.Linear(100, 10)                \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        hidden_activations = []\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        x = self.flat(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu3(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        out = self.last_linear(x)\n",
        "        hidden_activations.append(out)\n",
        "        \n",
        "        return out, hidden_activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHXddnGZIhdl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_medium(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(CNN_medium, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=0, stride=1)\n",
        "        self.relu1 = nn.ReLU() \n",
        "        self.conv2 = nn.Conv2d(32, 32, 4, padding=0, stride=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=0, stride=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv4 = nn.Conv2d(64, 64, 4, padding=0, stride=2)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.flat = Flatten()\n",
        "        self.linear1 = nn.Linear(64*5*5, 512)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(512, 512)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.last_linear = nn.Linear(512, 10)                \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        hidden_activations = []\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.flat(x)\n",
        "        hidden_activations.append(x)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu5(x)\n",
        "        hidden_activations.append(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu6(x)\n",
        "        hidden_activations.append(x)\n",
        "        \n",
        "        out = self.last_linear(x)\n",
        "        hidden_activations.append(out)\n",
        "        \n",
        "        return out, hidden_activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pczaC9Iwwy3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CNN_medium().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KBRQTG-zpQ_3"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "1GBAHYhdRVqp",
        "colab_type": "code",
        "outputId": "768c5035-2ca0-4a6f-8c26-738eb201d152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "for i in range(20):\n",
        "  train_err, _ = epoch(train_loader, model, device, opt)\n",
        "  print (train_err)\n",
        "\n",
        "test_err, _ = epoch(test_loader, model, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.60354\n",
            "0.46068\n",
            "0.39654\n",
            "0.35646\n",
            "0.3253\n",
            "0.30708\n",
            "0.28894\n",
            "0.28002\n",
            "0.26874\n",
            "0.2589\n",
            "0.2539\n",
            "0.24514\n",
            "0.2391\n",
            "0.23464\n",
            "0.23322\n",
            "0.22548\n",
            "0.22256\n",
            "0.221\n",
            "0.2186\n",
            "0.2134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFB7jFTyUkon",
        "colab_type": "code",
        "outputId": "165628fa-4dae-4765-e3ff-c15fd097aca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (test_err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QgFHqVWQpQ_4",
        "outputId": "b3c5aa21-d806-42ae-d2f5-42a85f0d5a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3621
        }
      },
      "cell_type": "code",
      "source": [
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPSILON = 8/255\n",
        "EPSILON_TRAIN = 8/255\n",
        "epsilon_schedule = generate_epsilon_schedule_CIFAR(EPSILON_TRAIN)\n",
        "kappa_schedule = generate_kappa_schedule_CIFAR()\n",
        "batch_counter = 0\n",
        "\n",
        "print(\"Epoch   \", \"Combined Loss\", \"MSE Loss\", \"Test Err\", \"Test Robust Err\", sep=\"\\t\")\n",
        "\n",
        "for t in range(350):\n",
        "    _, combined_loss, mse_loss = epoch_robust_bound(train_loader, model, epsilon_schedule, device, kappa_schedule, batch_counter, opt)\n",
        "    \n",
        "    # check loss and accuracy on test set\n",
        "    test_err, _ = epoch(test_loader, model, device)\n",
        "    robust_err = epoch_calculate_robust_err(test_loader, model, EPSILON, device)\n",
        "    \n",
        "    batch_counter += 1000\n",
        "    \n",
        "    if t == 200:  #decrease learning rate after 200 epochs\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group[\"lr\"] = 1e-4\n",
        "    \n",
        "    if t == 250:  #decrease learning rate after 250 epochs\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group[\"lr\"] = 1e-5\n",
        "    \n",
        "    if t == 300:  #decrease learning rate after 300 epochs\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "    \n",
        "    print(*(\"{:.6f}\".format(i) for i in (t, combined_loss, mse_loss, test_err, robust_err)), sep=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   \tCombined Loss\tMSE Loss\tTest Err\tTest Robust Err\n",
            "0.000000\t0.004207\t0.000007\t0.768200\t1.000000\n",
            "1.000000\t0.004033\t0.000048\t0.751700\t1.000000\n",
            "2.000000\t0.003908\t0.000034\t0.732000\t1.000000\n",
            "3.000000\t0.003840\t0.000038\t0.709100\t1.000000\n",
            "4.000000\t0.003864\t0.000042\t0.698100\t1.000000\n",
            "5.000000\t0.003800\t0.000046\t0.680800\t1.000000\n",
            "6.000000\t0.003805\t0.000049\t0.703300\t0.999400\n",
            "7.000000\t0.003781\t0.000050\t0.686400\t0.999900\n",
            "8.000000\t0.003763\t0.000051\t0.700700\t0.998300\n",
            "9.000000\t0.003740\t0.000051\t0.680700\t0.999000\n",
            "10.000000\t0.003720\t0.000060\t0.672400\t0.991300\n",
            "11.000000\t0.003725\t0.000060\t0.667800\t0.995400\n",
            "12.000000\t0.003684\t0.000066\t0.646200\t0.991600\n",
            "13.000000\t0.003637\t0.000074\t0.636000\t0.985600\n",
            "14.000000\t0.003619\t0.000074\t0.629300\t0.978100\n",
            "15.000000\t0.003605\t0.000078\t0.625000\t0.964500\n",
            "16.000000\t0.003592\t0.000075\t0.622600\t0.956500\n",
            "17.000000\t0.003529\t0.000087\t0.618000\t0.970200\n",
            "18.000000\t0.003490\t0.000093\t0.609800\t0.937900\n",
            "19.000000\t0.003516\t0.000085\t0.611200\t0.945000\n",
            "20.000000\t0.003506\t0.000085\t0.613900\t0.922000\n",
            "21.000000\t0.003540\t0.000090\t0.601200\t0.929900\n",
            "22.000000\t0.003483\t0.000095\t0.614400\t0.932500\n",
            "23.000000\t0.003501\t0.000092\t0.591200\t0.917700\n",
            "24.000000\t0.003493\t0.000091\t0.591100\t0.917500\n",
            "25.000000\t0.003544\t0.000088\t0.594900\t0.910100\n",
            "26.000000\t0.003500\t0.000097\t0.588900\t0.910500\n",
            "27.000000\t0.003508\t0.000104\t0.585900\t0.901600\n",
            "28.000000\t0.003476\t0.000098\t0.588100\t0.895900\n",
            "29.000000\t0.003500\t0.000101\t0.589500\t0.878400\n",
            "30.000000\t0.003441\t0.000105\t0.580900\t0.894200\n",
            "31.000000\t0.003480\t0.000106\t0.584100\t0.873900\n",
            "32.000000\t0.003439\t0.000104\t0.583300\t0.873300\n",
            "33.000000\t0.003449\t0.000107\t0.589700\t0.876800\n",
            "34.000000\t0.003468\t0.000112\t0.573200\t0.871400\n",
            "35.000000\t0.003460\t0.000113\t0.587200\t0.857300\n",
            "36.000000\t0.003456\t0.000113\t0.580900\t0.865800\n",
            "37.000000\t0.003451\t0.000113\t0.580100\t0.847400\n",
            "38.000000\t0.003435\t0.000112\t0.573900\t0.846800\n",
            "39.000000\t0.003482\t0.000116\t0.576000\t0.848700\n",
            "40.000000\t0.003456\t0.000116\t0.575600\t0.836300\n",
            "41.000000\t0.003439\t0.000115\t0.577400\t0.843700\n",
            "42.000000\t0.003467\t0.000120\t0.576000\t0.844600\n",
            "43.000000\t0.003468\t0.000121\t0.579000\t0.829400\n",
            "44.000000\t0.003460\t0.000124\t0.572800\t0.839400\n",
            "45.000000\t0.003457\t0.000123\t0.572500\t0.841900\n",
            "46.000000\t0.003417\t0.000127\t0.571700\t0.849700\n",
            "47.000000\t0.003485\t0.000124\t0.570100\t0.820100\n",
            "48.000000\t0.003466\t0.000128\t0.571900\t0.821200\n",
            "49.000000\t0.003430\t0.000132\t0.579900\t0.825600\n",
            "50.000000\t0.003474\t0.000129\t0.570800\t0.823100\n",
            "51.000000\t0.003427\t0.000133\t0.583100\t0.798300\n",
            "52.000000\t0.003462\t0.000132\t0.569900\t0.800100\n",
            "53.000000\t0.003484\t0.000137\t0.571800\t0.825300\n",
            "54.000000\t0.003461\t0.000138\t0.562200\t0.817700\n",
            "55.000000\t0.003463\t0.000134\t0.568500\t0.813000\n",
            "56.000000\t0.003452\t0.000138\t0.568700\t0.795400\n",
            "57.000000\t0.003437\t0.000147\t0.570500\t0.797800\n",
            "58.000000\t0.003425\t0.000141\t0.574200\t0.792800\n",
            "59.000000\t0.003417\t0.000147\t0.573300\t0.809500\n",
            "60.000000\t0.003455\t0.000144\t0.564900\t0.797300\n",
            "61.000000\t0.003445\t0.000146\t0.569500\t0.802700\n",
            "62.000000\t0.003467\t0.000135\t0.563200\t0.797700\n",
            "63.000000\t0.003494\t0.000151\t0.566900\t0.787800\n",
            "64.000000\t0.003466\t0.000145\t0.565100\t0.786400\n",
            "65.000000\t0.003431\t0.000145\t0.562600\t0.784400\n",
            "66.000000\t0.003457\t0.000148\t0.568700\t0.771600\n",
            "67.000000\t0.003453\t0.000143\t0.575000\t0.794000\n",
            "68.000000\t0.003478\t0.000147\t0.562100\t0.785700\n",
            "69.000000\t0.003443\t0.000152\t0.567300\t0.773200\n",
            "70.000000\t0.003419\t0.000162\t0.567300\t0.765100\n",
            "71.000000\t0.003471\t0.000151\t0.561600\t0.779800\n",
            "72.000000\t0.003508\t0.000154\t0.576800\t0.774600\n",
            "73.000000\t0.003442\t0.000161\t0.566600\t0.775100\n",
            "74.000000\t0.003446\t0.000158\t0.563600\t0.762600\n",
            "75.000000\t0.003469\t0.000156\t0.564800\t0.782800\n",
            "76.000000\t0.003474\t0.000155\t0.567000\t0.777900\n",
            "77.000000\t0.003485\t0.000149\t0.576900\t0.754600\n",
            "78.000000\t0.003487\t0.000153\t0.564600\t0.749100\n",
            "79.000000\t0.003493\t0.000157\t0.565700\t0.763800\n",
            "80.000000\t0.003420\t0.000162\t0.563200\t0.760000\n",
            "81.000000\t0.003456\t0.000158\t0.561600\t0.753900\n",
            "82.000000\t0.003452\t0.000161\t0.559200\t0.765900\n",
            "83.000000\t0.003508\t0.000153\t0.567300\t0.742500\n",
            "84.000000\t0.003511\t0.000155\t0.563900\t0.747600\n",
            "85.000000\t0.003477\t0.000160\t0.561300\t0.743600\n",
            "86.000000\t0.003477\t0.000158\t0.562800\t0.742400\n",
            "87.000000\t0.003449\t0.000161\t0.566400\t0.747400\n",
            "88.000000\t0.003507\t0.000158\t0.575500\t0.744300\n",
            "89.000000\t0.003509\t0.000159\t0.563900\t0.739800\n",
            "90.000000\t0.003500\t0.000147\t0.569600\t0.746200\n",
            "91.000000\t0.003512\t0.000153\t0.559500\t0.741100\n",
            "92.000000\t0.003478\t0.000151\t0.557600\t0.739200\n",
            "93.000000\t0.003496\t0.000154\t0.562100\t0.728500\n",
            "94.000000\t0.003474\t0.000156\t0.561900\t0.724400\n",
            "95.000000\t0.003495\t0.000158\t0.565300\t0.721800\n",
            "96.000000\t0.003525\t0.000157\t0.562500\t0.743000\n",
            "97.000000\t0.003488\t0.000168\t0.558300\t0.739100\n",
            "98.000000\t0.003493\t0.000156\t0.564500\t0.727100\n",
            "99.000000\t0.003506\t0.000165\t0.569600\t0.730600\n",
            "100.000000\t0.003507\t0.000165\t0.565300\t0.728000\n",
            "101.000000\t0.003508\t0.000165\t0.570800\t0.733300\n",
            "102.000000\t0.003500\t0.000168\t0.563900\t0.729200\n",
            "103.000000\t0.003465\t0.000167\t0.568000\t0.729800\n",
            "104.000000\t0.003492\t0.000171\t0.562200\t0.727800\n",
            "105.000000\t0.003541\t0.000158\t0.564000\t0.708200\n",
            "106.000000\t0.003528\t0.000167\t0.565400\t0.722500\n",
            "107.000000\t0.003525\t0.000162\t0.563300\t0.708400\n",
            "108.000000\t0.003513\t0.000162\t0.568200\t0.721500\n",
            "109.000000\t0.003542\t0.000166\t0.564300\t0.710200\n",
            "110.000000\t0.003512\t0.000167\t0.562400\t0.725600\n",
            "111.000000\t0.003534\t0.000169\t0.561600\t0.728500\n",
            "112.000000\t0.003559\t0.000165\t0.566300\t0.721000\n",
            "113.000000\t0.003540\t0.000163\t0.573100\t0.724200\n",
            "114.000000\t0.003546\t0.000162\t0.564600\t0.723200\n",
            "115.000000\t0.003549\t0.000166\t0.567200\t0.709800\n",
            "116.000000\t0.003521\t0.000159\t0.565000\t0.718000\n",
            "117.000000\t0.003546\t0.000160\t0.563900\t0.705900\n",
            "118.000000\t0.003588\t0.000157\t0.564300\t0.697600\n",
            "119.000000\t0.003545\t0.000163\t0.568000\t0.707000\n",
            "120.000000\t0.003567\t0.000162\t0.567400\t0.705300\n",
            "121.000000\t0.003532\t0.000163\t0.571900\t0.714600\n",
            "122.000000\t0.003563\t0.000171\t0.566700\t0.704200\n",
            "123.000000\t0.003542\t0.000169\t0.570200\t0.706500\n",
            "124.000000\t0.003567\t0.000163\t0.575400\t0.710700\n",
            "125.000000\t0.003547\t0.000167\t0.568400\t0.703100\n",
            "126.000000\t0.003592\t0.000167\t0.568400\t0.703600\n",
            "127.000000\t0.003570\t0.000166\t0.565500\t0.698700\n",
            "128.000000\t0.003569\t0.000162\t0.565200\t0.705400\n",
            "129.000000\t0.003578\t0.000171\t0.565000\t0.706600\n",
            "130.000000\t0.003607\t0.000164\t0.569200\t0.698700\n",
            "131.000000\t0.003573\t0.000169\t0.567900\t0.690700\n",
            "132.000000\t0.003568\t0.000167\t0.570600\t0.689700\n",
            "133.000000\t0.003595\t0.000156\t0.570100\t0.688800\n",
            "134.000000\t0.003560\t0.000168\t0.570900\t0.694400\n",
            "135.000000\t0.003570\t0.000168\t0.570700\t0.698400\n",
            "136.000000\t0.003614\t0.000172\t0.569900\t0.705800\n",
            "137.000000\t0.003587\t0.000167\t0.570800\t0.692100\n",
            "138.000000\t0.003587\t0.000159\t0.564400\t0.692200\n",
            "139.000000\t0.003596\t0.000175\t0.572800\t0.694500\n",
            "140.000000\t0.003567\t0.000170\t0.561900\t0.685800\n",
            "141.000000\t0.003559\t0.000168\t0.568300\t0.689300\n",
            "142.000000\t0.003618\t0.000165\t0.566400\t0.688700\n",
            "143.000000\t0.003596\t0.000164\t0.565300\t0.686600\n",
            "144.000000\t0.003613\t0.000162\t0.572400\t0.690900\n",
            "145.000000\t0.003598\t0.000168\t0.564000\t0.691500\n",
            "146.000000\t0.003623\t0.000169\t0.570100\t0.681300\n",
            "147.000000\t0.003627\t0.000165\t0.577300\t0.700200\n",
            "148.000000\t0.003643\t0.000163\t0.565200\t0.694000\n",
            "149.000000\t0.003604\t0.000165\t0.569300\t0.689400\n",
            "150.000000\t0.003591\t0.000164\t0.575500\t0.689300\n",
            "151.000000\t0.003602\t0.000169\t0.567400\t0.687500\n",
            "152.000000\t0.003580\t0.000166\t0.566100\t0.684500\n",
            "153.000000\t0.003611\t0.000165\t0.569400\t0.693000\n",
            "154.000000\t0.003629\t0.000167\t0.567000\t0.682100\n",
            "155.000000\t0.003623\t0.000158\t0.576300\t0.680900\n",
            "156.000000\t0.003558\t0.000162\t0.568900\t0.684300\n",
            "157.000000\t0.003595\t0.000159\t0.568400\t0.676900\n",
            "158.000000\t0.003586\t0.000159\t0.567800\t0.678900\n",
            "159.000000\t0.003591\t0.000162\t0.569100\t0.675800\n",
            "160.000000\t0.003605\t0.000155\t0.569700\t0.683400\n",
            "161.000000\t0.003630\t0.000159\t0.571700\t0.687300\n",
            "162.000000\t0.003649\t0.000154\t0.573900\t0.678000\n",
            "163.000000\t0.003595\t0.000163\t0.567300\t0.679700\n",
            "164.000000\t0.003618\t0.000157\t0.579400\t0.682000\n",
            "165.000000\t0.003619\t0.000166\t0.565600\t0.684900\n",
            "166.000000\t0.003618\t0.000165\t0.571600\t0.685700\n",
            "167.000000\t0.003568\t0.000167\t0.571900\t0.676300\n",
            "168.000000\t0.003606\t0.000157\t0.566300\t0.674500\n",
            "169.000000\t0.003621\t0.000156\t0.567400\t0.676800\n",
            "170.000000\t0.003579\t0.000159\t0.570700\t0.672800\n",
            "171.000000\t0.003601\t0.000157\t0.568000\t0.683500\n",
            "172.000000\t0.003597\t0.000163\t0.569300\t0.679600\n",
            "173.000000\t0.003622\t0.000164\t0.568900\t0.676600\n",
            "174.000000\t0.003614\t0.000157\t0.569500\t0.683600\n",
            "175.000000\t0.003583\t0.000161\t0.569100\t0.688700\n",
            "176.000000\t0.003628\t0.000164\t0.566300\t0.688700\n",
            "177.000000\t0.003645\t0.000159\t0.567300\t0.675900\n",
            "178.000000\t0.003603\t0.000160\t0.569400\t0.674000\n",
            "179.000000\t0.003587\t0.000151\t0.575200\t0.681300\n",
            "180.000000\t0.003602\t0.000152\t0.568600\t0.675700\n",
            "181.000000\t0.003603\t0.000157\t0.575400\t0.677100\n",
            "182.000000\t0.003605\t0.000161\t0.571200\t0.674100\n",
            "183.000000\t0.003604\t0.000160\t0.571900\t0.678200\n",
            "184.000000\t0.003587\t0.000148\t0.577600\t0.681700\n",
            "185.000000\t0.003610\t0.000149\t0.566900\t0.665600\n",
            "186.000000\t0.003637\t0.000153\t0.566200\t0.670900\n",
            "187.000000\t0.003602\t0.000149\t0.566700\t0.672200\n",
            "188.000000\t0.003605\t0.000156\t0.571500\t0.674800\n",
            "189.000000\t0.003602\t0.000158\t0.562400\t0.676700\n",
            "190.000000\t0.003623\t0.000158\t0.566700\t0.683800\n",
            "191.000000\t0.003624\t0.000148\t0.570000\t0.672900\n",
            "192.000000\t0.003656\t0.000143\t0.567100\t0.668800\n",
            "193.000000\t0.003610\t0.000149\t0.572000\t0.670200\n",
            "194.000000\t0.003608\t0.000152\t0.569600\t0.666000\n",
            "195.000000\t0.003621\t0.000146\t0.569900\t0.672000\n",
            "196.000000\t0.003623\t0.000143\t0.573800\t0.677800\n",
            "197.000000\t0.003614\t0.000147\t0.567600\t0.678300\n",
            "198.000000\t0.003621\t0.000161\t0.576200\t0.684500\n",
            "199.000000\t0.003626\t0.000143\t0.570000\t0.670500\n",
            "200.000000\t0.003606\t0.000151\t0.578600\t0.684700\n",
            "201.000000\t0.003596\t0.000134\t0.568600\t0.665300\n",
            "202.000000\t0.003566\t0.000133\t0.567400\t0.663400\n",
            "203.000000\t0.003565\t0.000133\t0.568200\t0.662500\n",
            "204.000000\t0.003530\t0.000134\t0.567100\t0.661800\n",
            "205.000000\t0.003495\t0.000131\t0.567400\t0.663600\n",
            "206.000000\t0.003514\t0.000139\t0.566300\t0.660900\n",
            "207.000000\t0.003540\t0.000132\t0.566600\t0.662400\n",
            "208.000000\t0.003516\t0.000138\t0.563900\t0.660800\n",
            "209.000000\t0.003488\t0.000140\t0.564400\t0.658600\n",
            "210.000000\t0.003544\t0.000137\t0.562900\t0.657900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d6sPZAF0y99d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}